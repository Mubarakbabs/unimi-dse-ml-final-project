{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.DecisionTree import DecisionTree\n",
    "from scripts.utils import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ucimlrepo import fetch_ucirepo #for importing data\n",
    "from summarytools import dfSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "secondary_mushroom = fetch_ucirepo(id=848) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X_loaded = secondary_mushroom.data.features \n",
    "y_loaded = secondary_mushroom.data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_loaded.copy()\n",
    "y = y_loaded.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(['spore-print-color', 'veil-type', 'veil-color', 'stem-root'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X:\n",
    "    X.loc[:,col]=X.loc[:,col].astype(str) if X.loc[:,col].dtype == 'object' else X.loc[:,col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class': {'e': 0, 'p': 1}}\n"
     ]
    }
   ],
   "source": [
    "#encode y into 0s and 1s. \n",
    "y_mapping = encode_labels(y)\n",
    "print(y_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import Optional\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature: str = None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=10, n_features=None, split_using=\"entropy\"):\n",
    "        if split_using not in ('entropy', 'gini', 'class_error'):\n",
    "            raise ValueError(f\"split_using argument must be one of ('entropy', 'gini', 'class_error')\")\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features\n",
    "        self.root = None\n",
    "        self.split_using = split_using\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)     \n",
    "        self.leaves = []\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1], self.n_features)\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_feats = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        if depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            self.leaves.append(leaf_value)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n",
    "        best_feature, best_thresh, best_gain = self._best_split(X, y, feat_idxs)\n",
    "        if best_gain == 0:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            self.leaves.append(leaf_value)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "        return Node(best_feature, best_thresh, left, right)\n",
    "\n",
    "    def _best_split(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "\n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_column.astype(str)) if self._iscategorical(X_column) else np.unique(X_column)\n",
    "            #thresholds = np.unique(X_column.astype(str)) if X_column.dtype == 'object' else np.unique(X_column)\n",
    "\n",
    "            for thr in thresholds:\n",
    "                gain = self._gain(y, X_column, thr)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = thr\n",
    "\n",
    "        return split_idx, split_threshold, best_gain\n",
    "    \n",
    "    def _iscategorical(self, X_column):\n",
    "        try:\n",
    "            X_column = X_column.astype(float)\n",
    "            return False\n",
    "        except ValueError:\n",
    "            return True\n",
    "\n",
    "\n",
    "    def _gain(self, y, X_column, threshold):\n",
    "        if self.split_using == 'entropy':\n",
    "            parent_entropy = self._entropy(y)\n",
    "            left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "\n",
    "            if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "                return 0\n",
    "\n",
    "            n = len(y)\n",
    "            n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "            e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
    "            child_entropy = (n_l / n) * e_l + (n_r / n) * e_r\n",
    "\n",
    "            information_gain = parent_entropy - child_entropy\n",
    "            return information_gain\n",
    "\n",
    "        elif self.split_using == 'gini':\n",
    "            parent_gini = self._gini(y)\n",
    "            left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "\n",
    "            if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "                return 0\n",
    "\n",
    "            n = len(y)\n",
    "            n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "            gini_l, gini_r = self._gini(y[left_idxs]), self._gini(y[right_idxs])\n",
    "            child_gini = (n_l / n) * gini_l + (n_r / n) * gini_r\n",
    "\n",
    "            information_gain = parent_gini - child_gini\n",
    "            return information_gain\n",
    "\n",
    "        elif self.split_using == 'class_error':\n",
    "            parent_error = self._class_error(y)\n",
    "            left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "\n",
    "            if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "                return 0\n",
    "\n",
    "            n = len(y)\n",
    "            n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "            error_l, error_r = self._class_error(y[left_idxs]), self._class_error(y[right_idxs])\n",
    "            child_error = (n_l / n) * error_l + (n_r / n) * error_r\n",
    "\n",
    "            information_gain = parent_error - child_error\n",
    "            return information_gain\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        try:\n",
    "            split_thresh = float(split_thresh)\n",
    "            X_column = X_column.astype(float)\n",
    "            left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "            right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "\n",
    "        except ValueError:\n",
    "            #since all thresholds have been converted into strings, for correct handling of nulls, X_column will need to be converted into strings too\n",
    "            split_thresh = str(split_thresh)\n",
    "            X_column = X_column.astype(str)\n",
    "            left_idxs = np.argwhere(X_column.astype(str) == split_thresh).flatten()\n",
    "            right_idxs = np.argwhere(X_column.astype(str) != split_thresh).flatten()\n",
    "\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _class_error(self, y):\n",
    "        counter = Counter(y.flatten())  # Convert to list\n",
    "        most_common_label, count = counter.most_common(1)[0]\n",
    "        return 1 - (count / len(y))\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        counts = np.bincount(y.flatten())\n",
    "        total_count = len(y)\n",
    "        probabilities = counts / total_count\n",
    "        \n",
    "        # Filter out zero probabilities to avoid log2(0)\n",
    "        probabilities = probabilities[probabilities > 0]\n",
    "        \n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities) + (1 - probabilities) * np.log2(1 - probabilities)) / 2\n",
    "        return entropy\n",
    "\n",
    "\n",
    "    def _gini(self, y):\n",
    "        counts = np.bincount(y.flatten())\n",
    "        total_count = len(y)\n",
    "        probabilities = counts / total_count\n",
    "        \n",
    "        # Filter out zero probabilities to avoid unnecessary calculations\n",
    "        probabilities = probabilities[probabilities > 0]\n",
    "        \n",
    "        gini = 2 * np.sum(probabilities * (1 - probabilities))\n",
    "        return gini\n",
    "\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        value = counter.most_common(1)[0][0]\n",
    "        return value\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "        return y_pred\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "        if type(x[node.feature]) == str:\n",
    "            if x[node.feature] == node.threshold:\n",
    "                return self._traverse_tree(x, node.left)\n",
    "\n",
    "            return self._traverse_tree(x, node.right)\n",
    "        else:\n",
    "            if x[node.feature] <= node.threshold:\n",
    "                return self._traverse_tree(x, node.left)\n",
    "\n",
    "            return self._traverse_tree(x, node.right)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the tree using each of the three splitting methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1: Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mubarak Babslawal\\AppData\\Local\\Temp\\ipykernel_6756\\4028717886.py:156: RuntimeWarning: divide by zero encountered in log2\n",
      "  entropy = -np.sum(probabilities * np.log2(probabilities) + (1 - probabilities) * np.log2(1 - probabilities)) / 2\n",
      "C:\\Users\\Mubarak Babslawal\\AppData\\Local\\Temp\\ipykernel_6756\\4028717886.py:156: RuntimeWarning: invalid value encountered in multiply\n",
      "  entropy = -np.sum(probabilities * np.log2(probabilities) + (1 - probabilities) * np.log2(1 - probabilities)) / 2\n"
     ]
    }
   ],
   "source": [
    "#training the decision tree\n",
    "entropy_model = DecisionTree(split_using='entropy', max_depth=10)\n",
    "entropy_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.34%\n",
      "0.9033895529719993\n",
      "Precision: 93.04%\n",
      "0.930352798053528\n",
      "Recall: 89.44%\n",
      "0.8944444444444445\n"
     ]
    }
   ],
   "source": [
    "#performance of entropy model\n",
    "entropy_pred = entropy_model.predict(X_test)\n",
    "accuracy(y_test, entropy_pred)\n",
    "precision(y_test, entropy_pred)\n",
    "recall(y_test, entropy_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2: Gini impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the decision tree\n",
    "gini_model = DecisionTree(split_using='gini', max_depth=10)\n",
    "gini_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.43%\n",
      "Precision: 95.45%\n",
      "Recall: 87.06%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8706140350877193"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#performance of gini model\n",
    "gini_pred = gini_model.predict(X_test)\n",
    "accuracy(y_test, gini_pred)\n",
    "precision(y_test, gini_pred)\n",
    "recall(y_test, gini_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3: Train Error\n",
    "\n",
    "Next we adopt the training error using zero-one loss as a splitting criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the decision tree\n",
    "class_error_model = DecisionTree(split_using='class_error', max_depth=10)\n",
    "class_error_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.61%\n",
      "Precision: 87.78%\n",
      "Recall: 75.95%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7595029239766082"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#performance of class_error model\n",
    "class_error_pred = class_error_model.predict(X_test)\n",
    "accuracy(y_test, class_error_pred)\n",
    "precision(y_test, class_error_pred)\n",
    "recall(y_test, class_error_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy train error: 0.09041039811687647\n",
      "gini train error: 0.09415617644048715\n",
      "class_error train error: 0.18579469859789172\n"
     ]
    }
   ],
   "source": [
    "#training errors of each model\n",
    "entropy_train = entropy_model.predict(X_train)\n",
    "gini_train = gini_model.predict(X_train)\n",
    "class_error_train = class_error_model.predict(X_train)\n",
    "print(f\"entropy train error: {zero_one_loss(y_train, entropy_train)}\")\n",
    "print(f\"gini train error: {zero_one_loss(y_train, gini_train)}\")\n",
    "print(f\"class_error train error: {zero_one_loss(y_train, class_error_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to see that the training errors are similar to the test errors, showing that the model didn't overfit considering the stopping criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally we perform hyper parameter tuning to optimize the model on the max depth stopping criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree depth: 10, validation error: 0.13632176849861835\n",
      "tree depth: 11, validation error: 0.09620304984136731\n",
      "tree depth: 12, validation error: 0.06273666973697677\n",
      "tree depth: 13, validation error: 0.04175621737795517\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#tryna see if at some point the validation starts to rise\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#tuning the tree on the max_depth criterion\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m tuned_tree \u001b[38;5;241m=\u001b[39m \u001b[43mtune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_depth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msplit_using\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgini\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mubarak Babslawal\\OneDrive - Università degli Studi di Milano\\UniMi\\1-2\\Machine Learning Statistics\\unimi-dse-ml-final-project\\scripts\\final\\utils.py:88\u001b[0m, in \u001b[0;36mtune\u001b[1;34m(X, y, tune_on, split_using, start, stop)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Mubarak Babslawal\\OneDrive - Università degli Studi di Milano\\UniMi\\1-2\\Machine Learning Statistics\\unimi-dse-ml-final-project\\scripts\\final\\DecisionTree.py:32\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Mubarak Babslawal\\OneDrive - Università degli Studi di Milano\\UniMi\\1-2\\Machine Learning Statistics\\unimi-dse-ml-final-project\\scripts\\final\\DecisionTree.py:51\u001b[0m, in \u001b[0;36m_grow_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Mubarak Babslawal\\OneDrive - Università degli Studi di Milano\\UniMi\\1-2\\Machine Learning Statistics\\unimi-dse-ml-final-project\\scripts\\final\\DecisionTree.py:51\u001b[0m, in \u001b[0;36m_grow_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Mubarak Babslawal\\OneDrive - Università degli Studi di Milano\\UniMi\\1-2\\Machine Learning Statistics\\unimi-dse-ml-final-project\\scripts\\final\\DecisionTree.py:51\u001b[0m, in \u001b[0;36m_grow_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Mubarak Babslawal\\OneDrive - Università degli Studi di Milano\\UniMi\\1-2\\Machine Learning Statistics\\unimi-dse-ml-final-project\\scripts\\final\\DecisionTree.py:44\u001b[0m, in \u001b[0;36m_grow_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Mubarak Babslawal\\OneDrive - Università degli Studi di Milano\\UniMi\\1-2\\Machine Learning Statistics\\unimi-dse-ml-final-project\\scripts\\final\\DecisionTree.py:65\u001b[0m, in \u001b[0;36m_best_split\u001b[1;34m(self, X, y, feat_idxs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Mubarak Babslawal\\OneDrive - Università degli Studi di Milano\\UniMi\\1-2\\Machine Learning Statistics\\unimi-dse-ml-final-project\\scripts\\final\\DecisionTree.py:99\u001b[0m, in \u001b[0;36m_gain\u001b[1;34m(self, y, X_column, threshold)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Mubarak Babslawal\\OneDrive - Università degli Studi di Milano\\UniMi\\1-2\\Machine Learning Statistics\\unimi-dse-ml-final-project\\scripts\\final\\DecisionTree.py:132\u001b[0m, in \u001b[0;36m_split\u001b[1;34m(self, X_column, split_thresh)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Mubarak Babslawal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\numeric.py:608\u001b[0m, in \u001b[0;36margwhere\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    606\u001b[0m     \u001b[38;5;66;03m# then remove the added dimension\u001b[39;00m\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m argwhere(a)[:,:\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transpose(\u001b[43mnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Mubarak Babslawal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1973\u001b[0m, in \u001b[0;36mnonzero\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m   1881\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_nonzero_dispatcher)\n\u001b[0;32m   1882\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnonzero\u001b[39m(a):\n\u001b[0;32m   1883\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;124;03m    Return the indices of the elements that are non-zero.\u001b[39;00m\n\u001b[0;32m   1885\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1971\u001b[0m \n\u001b[0;32m   1972\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnonzero\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mubarak Babslawal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#tryna see if at some point the validation starts to rise\n",
    "#tuning the tree on the max_depth criterion\n",
    "\n",
    "tuned_tree = tune(X_train, y_train, tune_on='max_depth',split_using='gini', start=10, stop=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TunedTree:\n",
    "    def __init__(self, split_using, n_shuffles, start, stop, n_jobs=-1 ) -> None:\n",
    "        self.split_using = split_using \n",
    "        self.n_shuffles = n_shuffles\n",
    "        self.start = start\n",
    "        self.stop = stop\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "\n",
    "    def evaluate_model(X_train, y_train, X_validate, y_validate, tune_on, split_using, i, cv_index):\n",
    "        params = {tune_on: i, 'split_using': split_using}\n",
    "        tree = DecisionTree(**params)\n",
    "        X_train = np.random.permutation\n",
    "        tree.fit(X_train, y_train)\n",
    "        y_pred = tree.predict(X_validate)\n",
    "        validation_error = zero_one_loss(y_validate, y_pred)\n",
    "        print(f\"tree depth: {tree.max_depth}, , cv_index: {cv_index}, validation error: {validation_error}\")\n",
    "        return validation_error, i\n",
    "\n",
    "    def _tune(X, y, tune_on, split_using, n_shuffles, start, stop, n_jobs=-1):\n",
    "        for cv_index in range(n_shuffles):\n",
    "            permuted_indices = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X[permuted_indices]\n",
    "            y_shuffled = y[permuted_indices]\n",
    "            X_train, X_validate, y_train, y_validate = train_test_split(X_shuffled, y_shuffled, test_size=0.2)\n",
    "        \n",
    "            results = Parallel(n_jobs=n_jobs)(\n",
    "                delayed(evaluate_model)(X_train, y_train, X_validate, y_validate, tune_on, split_using, n_shuffles, i)\n",
    "                for i in range(start, stop)\n",
    "            )\n",
    "        \n",
    "        best_error, best_depth = min(results, key=lambda x: x[0])\n",
    "        \n",
    "        print(f\"best depth: {best_depth} split criterion: {split_using} validation error: {round(best_error * 100, 2)} %\")\n",
    "        return [(f\"tree_depth: {i}\", f\"validation error:{error}\") for error, i in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from scripts.utils import *\n",
    "from scripts.DecisionTree import DecisionTree\n",
    "\n",
    "def evaluate_model(X_train, y_train, X_validate, y_validate, tune_on, split_using, i, cv_index):\n",
    "    params = {tune_on: i, 'split_using': split_using}\n",
    "    tree = DecisionTree(**params)\n",
    "    tree.fit(X_train, y_train)\n",
    "    y_pred = tree.predict(X_validate)\n",
    "    validation_error = zero_one_loss(y_validate, y_pred)\n",
    "    print(f\"Tree depth: {tree.max_depth}, Shuffle index: {cv_index}, Validation error: {validation_error}\")\n",
    "    return validation_error, i\n",
    "\n",
    "def tune(X, y, tune_on, split_using, start, stop, n_shuffles=5, n_jobs=-1):\n",
    "    results_per_depth = {i: [] for i in range(start, stop)}\n",
    "    \n",
    "    for cv_index in range(n_shuffles):\n",
    "        # Shuffle the dataset\n",
    "        permuted_indices = np.random.permutation(X.shape[0])\n",
    "        X_shuffled = X[permuted_indices]\n",
    "        y_shuffled = y[permuted_indices]\n",
    "        \n",
    "        # Split the shuffled data into training and validation sets\n",
    "        X_train, X_validate, y_train, y_validate = train_test_split(X_shuffled, y_shuffled, test_size=0.2)\n",
    "        \n",
    "        # Evaluate the model for each depth\n",
    "        results = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(evaluate_model)(X_train, y_train, X_validate, y_validate, tune_on, split_using, i, cv_index)\n",
    "            for i in range(start, stop)\n",
    "        )\n",
    "        print(results)\n",
    "        \n",
    "        # Store the results for this shuffle\n",
    "        for error, depth in results:\n",
    "            results_per_depth[depth].append(error)\n",
    "        \n",
    "    \n",
    "    # Compute mean validation error for each depth\n",
    "    mean_errors = {depth: np.mean(errors) for depth, errors in results_per_depth.items()}\n",
    "    \n",
    "    best_depth = min(mean_errors, key=mean_errors.get)\n",
    "    best_error = mean_errors[best_depth]\n",
    "    \n",
    "    print(f\"Best depth: {best_depth}, Split criterion: {split_using}, Mean validation error: {round(best_error * 100, 2)} %\")\n",
    "    \n",
    "    # Format results for return\n",
    "    return [(f\"tree_depth: {depth}\", f\"mean_validation_error: {error}\") for depth, error in mean_errors.items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3900317265377136, 1), (0.34510285538839425, 2), (0.3131716303346638, 3), (0.3008903899293829, 4)]\n",
      "[(0.3932043803090779, 1), (0.3485825401698905, 2), (0.31736772080646813, 3), (0.3059052297615392, 4)]\n",
      "[(0.39044110121788966, 1), (0.34694504144918636, 2), (0.31951693787739227, 3), (0.30201617029986694, 4)]\n",
      "[(0.38604032340599737, 1), (0.3445911370381742, 2), (0.31368334868488384, 3), (0.28072868693071334, 4)]\n",
      "[(0.40047078088220245, 1), (0.35902159451437926, 2), (0.3280114624910449, 3), (0.3149114727254119, 4)]\n",
      "Best depth: 4, Split criterion: gini, Mean validation error: 30.09 %\n"
     ]
    }
   ],
   "source": [
    "tuned_tree = tune(X_train,y_train, 'max_depth', split_using='gini', start=1, stop=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tree_depth: 1', 'mean_validation_error: 0.3920376624705762'),\n",
       " ('tree_depth: 2', 'mean_validation_error: 0.3488486337120049'),\n",
       " ('tree_depth: 3', 'mean_validation_error: 0.3183502200388906'),\n",
       " ('tree_depth: 4', 'mean_validation_error: 0.30089038992938283')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retraining the tree with the best validation error\n",
    "best_tree = DecisionTree(max_depth=20, split_using='gini')\n",
    "best_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.997789421974783\n",
      "Precision: 0.9980991373007749\n",
      "Recall: 0.997953216374269\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_tree.predict(X_test)\n",
    "print(accuracy(y_test, y_pred))\n",
    "print(precision(y_test, y_pred))\n",
    "print(recall(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
